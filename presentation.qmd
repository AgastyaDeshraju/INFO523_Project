---
title: "Airline Satisfaction Analysis"
subtitle: "INFO 523 - Fall 2024 - Project Final"
author: "Nick Ferrante, Jasdeep Singh Jhajj, Agastya Deshraju"
title-slide-attributes:
  data-background-image: images/airplane.png
  data-background-size: stretch
  data-background-opacity: "0.7"
  data-slide-number: none
format:
  revealjs: 
    theme:  [beige,data/customtheming.scss]
    transition: concave
    background-transition: fade


editor: visual
execute:
  echo: false
---

```{r setup, echo=FALSE}
#| label: setup
#| include: false

# Plot theme
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 11))

# For better figure resolution
knitr::opts_chunk$set(
  fig.retina = 3,
  dpi = 300,
  #fig.width = 6, 
  fig.asp = 0.618 
  )
```

```{r load_pkgs, echo=FALSE}
#| label: load_pkgs
#| message: false
#| warning: false
#| code-summary: "Load Packages"

if (!require("pacman")) 
  install.packages("pacman")

pacman::p_load(tidyverse,
               dplyr,
               scales,
               arules,
               arulesViz,
               viridis,
               reshape2,
               cluster,
               dbscan,
               e1071,
               Rtsne,
               caret,
               rpart,
               randomForest,
               keras,
               dplyr,
               e1071,
               rpart,
               randomForest
) 
```

```{r}
data <- read.csv('train.csv')
data[is.na(data)] <- 0
data$satisfaction <- ifelse(data$satisfaction == "satisfied", "satisfied", "unsatisfied")
data$satisfaction_numerical <- ifelse(data$satisfaction == "satisfied", 1, 0)
numeric_data <- data[, -c(1,2,3,4,6,7,25)]

```

## Association
* In data mining, association rules identify patterns in a dataset by linking a set of items(anticedents) to another set of items(consequent)
* Antecedent (if) -> Consequent (then)

## Support and Confidence

* Support measures the frequency of an item in the dataset
* Confidence measures the reliability of the rule
* Lift measures the strength of an association 

## Heatmap Correlation

```{r}
cor_matrix <- cor(numeric_data, use = "complete.obs")

cor_melted <- melt(cor_matrix)

ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "#d6eaf8", high = "#4f86c1") + # Blue-to-Green gradient
  labs(
    title = "Correlation Matrix Heatmap",
    x = "Features",
    y = "Features",
    fill = "Correlation"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

```

## Heatmap interpretation

-   There's a strong correlation between *Seat Comfort, Inflight Entertainment, Online Boarding* with *Satisfaction*
-   While operating metrics like *Delays* negatively affect customer satisfaction, their impact is relatively weaker compared to customer-facing metrics

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Prepare the dataset
data_temp <- data

# Define the columns to be recoded
columnsSQE <- c("Online.boarding", "Seat.comfort", "Inflight.entertainment", 
                "Leg.room.service", "On.board.service", "Cleanliness", 
                "Inflight.wifi.service", "Baggage.handling")

# Recode numeric columns into factors: "Poor", "Neutral", "Good"
recode_to_factors <- function(column) {
  factor(ifelse(column %in% c(1, 2), "Poor",
         ifelse(column == 3, "Neutral", "Good")),
         levels = c("Poor", "Neutral", "Good"))
}

for(col in columnsSQE){
  data_temp[[col]] <- recode_to_factors(data_temp[[col]])
}

# Add a new column for satisfaction as a factor
data_temp$satisfaction <- factor(ifelse(data_temp$satisfaction_numerical == 1, 
                                        "Satisfied", "Unsatisfied"),
                                  levels = c("Unsatisfied", "Satisfied"))

# Include the satisfaction column in the subset for transactions
columnsSQE <- c(columnsSQE, "satisfaction")

# Subset only the selected columns for association rule mining
data_subset <- data_temp[, columnsSQE]

# Convert to transactions format
transactions <- as(data_subset, "transactions")
```



## Apriori

* Uses a generate-and-test approach.
* Follows the apriori property

```{r echo=FALSE, include=FALSE, warning=FALSE}
# Run the Apriori algorithm
ars_adjusted <- apriori(transactions, parameter = list(support = 0.02, confidence = 0.6))

# Subset rules where satisfaction = Satisfied is in the rhs
satisfied_rules <- subset(ars_adjusted, rhs %in% "satisfaction=Satisfied")

# Extract top 10 rules by Lift
top_rules_lift <- sort(satisfied_rules, by = "lift")[1:10]
```




## Scatter plot of Supportt vs Confidence for Satisfaction
```{r  warning=FALSE}
plot(
  satisfied_rules,
  method = "scatterplot",
  measure = c("support", "confidence"),
  shading = "lift",
  control = list(
    col = colorRampPalette(c( "#4f86c1", "#d6eaf8"))(1.8)
  )
)
```
## Interpretation of Support Vs Confidence for Satisfaction
* The graph shows a concentration of the higher lift points around the high confidence - low support region
* When selecting the rules, efforts should be made to balance high support rules, with high confidence and high lift rules


## Satisfaction based Rule Network Graph

<iframe src="network_plot_satisfied.html" width="100%" height="550-0px" frameborder="0"></iframe>


```{r warning=FALSE}
# plot(
#   top_rules_lift,
#   method = "graph",
#   engine = "htmlwidget",
#   control = list(
#     nodeCol = "#4ebded",  # Node color (blue-green)
#     edgeCol = "#4f86c1",  # Edge color (light blue)
#     alpha = 0.8           # Transparency for edges
#   )
# )

```
## Interpretation of the network graph

-   Positive features like *Seat comfort, Inflight entertainment, Online boarding, Leg room service* and *On-board service* are connected to multiple rules indicating that they play significant roles in driving satisfaction.
-   Features such as *Baggage handling* and *Cleanliness* play important roles as well but not as much as the ones mentioned before.



## FP Growth

* Uses a divide-and-conquer approach
* Constructs a compact data structure to represent the data

```{r echo=FALSE, include=FALSE, warning=FALSE}
fp_itemsets <- apriori(transactions, parameter = list(support = 0.02, target = "frequent itemsets"))

# Generate rules from the frequent itemsets
fp_rules <- apriori(transactions, parameter = list(support = 0.02, confidence = 0.6))

# Filter rules where satisfaction = Satisfied is in the RHS
unsatisfaction_rules <- subset(fp_rules, rhs %in% "satisfaction=Unsatisfied")

```


## Scatter plot of Support Vs Confidence for Unsatisfaction
```{r warning=FALSE}
# Scatterplot for satisfaction rules
plot(
  unsatisfaction_rules, 
  method = "scatterplot", 
  measure = c("support", "confidence"), 
  shading = "lift",
  control = list(
    col = colorRampPalette(c( "#4f86c1", "#d6eaf8"))(1.8)
  )
)
```
## Interpretation of Support Vs Confidence for Unsatisfaction
* Similar to the satisfaction plot, there is a high concentration of low support and high confidence points.
* Focus should therefore be drawn to rules with moderate lift and higher support .


```{r echo=FALSE, message=FALSE, warning=FALSE}
top_unsatisfaction_rules <- sort(unsatisfaction_rules, by = "lift")[1:10]
```

## Unsatisfaction based Rule Network Graph

<iframe src="network_plot_unsatisfied_fp.html" width="100%" height="550-0px" frameborder="0"></iframe>


```{r}
# plot(
#   top_unsatisfaction_rules, 
#   method = "graph", 
#   engine = "htmlwidget",
#   control = list(
#     nodeCol = "#4ebded",  # Node color (blue-green)
#     edgeCol = "#4f86c1",  # Edge color (light blue)
#     alpha = 0.8           # Transparency for edges
#   )
# )
```

## Interpretation of the Network Graph

* Features such as Poor Leg Service and Poor Wi-Fi Service contribute to unsatisfaction
* Thicker and darker edges represent stronger rules


## Classification Introduction

- This presentation showcases the performance of three classification models: **SVM**, **Decision Tree**, and **Random Forest**.
- For each model, we:
  - Select relevant features based on correlation analysis.
  - Train the model.
  - Evaluate performance using a confusion matrix and accuracy.
  - Visualize results with relevant plots.

```{r, echo=FALSE}
library(tidyverse)
library(caret)
library(e1071)    # For SVM
library(rpart)    # For Decision Tree
library(randomForest) # For Random Forest
library(dplyr)
# Load the training dataset
train_dataset <- read.csv("train.csv")

# Convert "satisfaction" column to factor
train_dataset$satisfaction <- as.factor(train_dataset$satisfaction)

# Handle missing values if any
train_dataset <- train_dataset %>% drop_na()

# Convert categorical variables to factors
categorical_cols <- c("Gender", "Customer.Type", "Type.of.Travel", "Class")
train_dataset[categorical_cols] <- lapply(train_dataset[categorical_cols], as.factor)

# Split data into features and labels
train_x <- train_dataset %>% select(-satisfaction)
train_y <- train_dataset$satisfaction

# Load the test dataset
test_dataset <- read.csv("test.csv")

# Convert "satisfaction" column to factor if it exists
if ("satisfaction" %in% colnames(test_dataset)) {
  test_dataset$satisfaction <- as.factor(test_dataset$satisfaction)
}

# Handle missing values if any
test_dataset <- test_dataset %>% drop_na()

# Convert categorical variables to factors
test_dataset[categorical_cols] <- lapply(test_dataset[categorical_cols], as.factor)

# Split data into features and labels
test_x <- test_dataset %>% select(-satisfaction)
test_y <- if ("satisfaction" %in% colnames(test_dataset)) test_dataset$satisfaction else NULL

```
## Support Vector Machine (SVM)

- SVM uses a linear kernel to classify satisfaction based on selected features.
- Features selected: `Inflight.wifi.service`, `Flight.Distance`, `Online.boarding` and `Seat.comfort`.

```{r}
# Select relevant features
selected_features <- c("Inflight.wifi.service", "Flight.Distance",
                       "Online.boarding", "Seat.comfort", "satisfaction")

# Create subsets for training and testing
# train_dataset_reduced <- train_dataset[, selected_features]
# test_dataset_reduced <- test_dataset[, selected_features]
# 
# # Train SVM model
# svm_model <- svm(satisfaction ~ ., data = train_dataset_reduced, kernel = "linear")
# 
# # Predict and evaluate
# svm_predictions <- predict(svm_model, test_dataset_reduced)
# confusionMatrix(svm_predictions, test_y)
```


## SVM: Confusion Matrix Heatmap

```{r}
library(ggplot2)
library(caret)
library(reshape2)

# # Generate confusion matrix for SVM
# conf_matrix_svm <- confusionMatrix(svm_predictions, test_y)$table
# conf_matrix_svm_melted <- melt(conf_matrix_svm)
# 
# # Heatmap
# ggplot(conf_matrix_svm_melted, aes(x = Prediction, y = Reference, fill = value)) +
#   geom_tile(color = "white") +
#   scale_fill_gradient(low = "lightblue", high = "darkblue") +
#   labs(title = "Confusion Matrix (SVM)", x = "Predicted", y = "Actual", fill = "Frequency") +
#   theme_minimal()
```


## Decision Tree

- Decision Tree classifies satisfaction by recursively splitting the dataset.
- Uses `rpart` library for model creation and visualization.

```{r}
library(rpart)
library(rpart.plot)

# Fit Decision Tree model
tree_model <- rpart(satisfaction ~ ., data = train_dataset, method = "class")

# Predict and evaluate
tree_predictions <- predict(tree_model, test_x, type = "class")
confusionMatrix(tree_predictions, test_y)
```


## Decision Tree: Visualization

```{r}
# Plot the decision tree
rpart.plot(tree_model, type = 4, extra = 104, main = "Decision Tree for Satisfaction")
```


## Random Forest

- Random Forest combines multiple decision trees for robust classification.
- Utilizes `randomForest` library to build the model.

```{r}
library(randomForest)

# Train Random Forest model
rf_model <- randomForest(satisfaction ~ ., data = train_dataset, ntree = 100)

# Predict and evaluate
rf_predictions <- predict(rf_model, test_x)
confusionMatrix(rf_predictions, test_y)
```


## Random Forest: Confusion Matrix Heatmap

```{r}
# Generate confusion matrix for Random Forest
conf_matrix_rf <- confusionMatrix(rf_predictions, test_y)$table
conf_matrix_rf_melted <- melt(conf_matrix_rf)

# Heatmap
ggplot(conf_matrix_rf_melted, aes(x = Prediction, y = Reference, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Confusion Matrix (Random Forest)", x = "Predicted", y = "Actual", fill = "Frequency") +
  theme_minimal()
```


## Feature Importance: Random Forest

- Identifies the features contributing most to classification decisions.

```{r}
# Feature importance plot
importance_df <- data.frame(Feature = rownames(importance(rf_model)), Importance = importance(rf_model)[, 1])

# Bar plot of feature importance
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance from Random Forest", x = "Features", y = "Importance") +
  theme_minimal()
```


## Accuracy Comparison

- The following bar plot compares the accuracy of all three models.

```{r}
# Accuracy comparison
data.frame(
  Model = c("Decision Tree", "Random Forest"),
  Accuracy = c(
    #mean(svm_predictions == test_y),
    mean(tree_predictions == test_y),
    mean(rf_predictions == test_y)
  )
) %>%
  ggplot(aes(x = reorder(Model, -Accuracy), y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity", color = "black") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Model Accuracy Comparison", x = "Model", y = "Accuracy") +
  theme_minimal()
```

## Conclusion

- **SVM** performs well with fewer features.
- **Decision Tree** provides easy interpretability.
- **Random Forest** excels in feature importance and overall accuracy.
- Consider model choice based on application requirements and computational resources.
