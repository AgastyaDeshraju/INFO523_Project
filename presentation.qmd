---
title: "Airline Satisfaction Analysis"
subtitle: "INFO 523 - Fall 2024 - Project Final"
author: "Nick Ferrante, Jasdeep Singh Jhajj, Agastya Deshraju"
title-slide-attributes:
  data-background-image: images/airplane.png
  data-background-size: stretch
  data-background-opacity: "0.7"
  data-slide-number: none
format:
  revealjs: 
    theme:  [beige,data/customtheming.scss]
    transition: concave
    background-transition: fade


editor: visual
execute:
  echo: false
---

```{r setup, echo=FALSE}
#| label: setup
#| include: false

# Plot theme
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 11))

# For better figure resolution
knitr::opts_chunk$set(
  fig.retina = 3,
  dpi = 300,
  #fig.width = 6, 
  fig.asp = 0.618 
  )
```

```{r load_pkgs, echo=FALSE}
#| label: load_pkgs
#| message: false
#| warning: false
#| code-summary: "Load Packages"

if (!require("pacman")) 
  install.packages("pacman")

pacman::p_load(tidyverse,
               dplyr,
               scales,
               arules,
               arulesViz,
               viridis,
               reshape2,
               cluster,
               dbscan,
               e1071,
               Rtsne,
               caret,
               rpart,
               randomForest,
               keras,
               dplyr,
               e1071,
               rpart,
               randomForest
) 
```

```{r}
data <- read.csv('train.csv')
data[is.na(data)] <- 0
data$satisfaction <- ifelse(data$satisfaction == "satisfied", "satisfied", "unsatisfied")
data$satisfaction_numerical <- ifelse(data$satisfaction == "satisfied", 1, 0)
numeric_data <- data[, -c(1,2,3,4,6,7,25)]

```

## About the Dataset

-   Contains information about a variety of airlines regarding the satisfaction of their customers
-   23 variables including gender, age, class, flight distance, along with a variety of service satisfaction variables and an overall satisfaction level

## Clustering Analysis

```{r load_clustering_data}

clusterDf <- read.csv("clusterDf.csv")
clusterDf <- clusterDf |>
  select(-X) |>
  mutate(Class = factor(Class),
         satisfaction = factor(satisfaction))

```

-   Aims to identify patterns regarding the satisfaction of passengers based on their age, their flight experience relative to their flying class, along with their flight distance
-   Variables: Age, Class, Flight.Distance, Satisfaction
-   Algorithms: PAM and OPTICS

## PAM Clustering

-   Used the gower distance metric to accommodate the mixed variable types
-   Performed a log transformation on Flight.Distance variable to eliminate skewness

## Determining Number of Clusters

```{r calculate_distance_matrix, include=FALSE}

# Compute the distance matrix
gower_dist <- daisy(clusterDf,
                    metric = "gower",
                    type = list(logratio = 3))
gower_mat <- as.matrix(gower_dist)
summary(gower_dist)
```

```{r plot_silhouette_width, fig.width=6, fig.asp=0.618}
#| fig.align: "center"



# Calculate Silhouette width for 2 to 10 clusters with PAM
sil <- c(NA)
for(i in 2:10){
  pam_fit <- pam(gower_mat, diss = TRUE, k = i)
  sil[i] <- pam_fit$silinfo$avg.width
}

# Plot Silhouette width
silWidthData <- data.frame(2:10, sil[-1])
colnames(silWidthData) <- c("numClusters", "silWidth")

silWidthData |>
  ggplot(aes(x = numClusters, y = silWidth)) +
  geom_point(color = "blue", size = 3) +
  geom_line(color = "blue", lwd = 1) +
  scale_x_continuous(breaks = seq(1,10, by = 1)) +
  labs(x = "Number of Clusters",
       y = "Silhouette Width",
       title = "Pam Silhouette Scores for Different Cluster Amounts") +
  theme(panel.grid.minor = element_blank(),
        axis.title.x = element_text(margin = margin(t = 15)),
        axis.title.y = element_text(margin = margin(r = 15)),
        axis.text = element_text(size = 12))

```

## Examining the PAM Mediods

```{r pam-mediods}
# Perform clustering using PAM with 5 clusters
pam_fit <- pam(gower_dist, diss = TRUE, k = 5)
clusterDf$pam_cluster <- pam_fit$cluster

mediodsTable <- clusterDf[,-5]
colnames(mediodsTable) <- c("Age", 
                            "Class", 
                            "Flight Distance",
                            "Satisfaction")

knitr::kable(mediodsTable[pam_fit$medoids,], row.names = FALSE)
```

## Visualizing Clusters

```{r t-SNE_plot, fig.width=6, fig.asp=0.618}
tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)
tsne_data <- tsne_obj$Y |>
  data.frame() |>
  setNames(c("X", "Y")) |>
  mutate(cluster = factor(pam_fit$clustering))

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster)) +
  labs(x = "X",
       y = "Y",
       title = "t-SNE Plot of PAM Custering",
       color = "Cluster")
```

## OPTICS Clustering

::: panel-tabset
#### OPTICS

```{r optics-clustering, include=FALSE}
(res_op <- optics(gower_dist, eps = 10, minPts = 15))
```

```{r optics-reachability-plot}
plot(res_op)
```

#### DBSCAN

```{r extract-dbscan, include=FALSE}
(res_op_d <- extractDBSCAN(res_op, eps_cl = 0.05))
```

```{r dbscan-reachability-plot}
plot(res_op_d)
```
:::

## Analyzing OPTICS Cluster Values

```{r optics-cluster-eval, include=FALSE}
# Examine the average value for each variable from the OPTICS clustering

clusterDf$optics_cluster <- res_op_d$cluster

# Find the average value for each numeric variable based on cluster
clusterAvg <- clusterDf |>
  group_by(optics_cluster) |>
  summarize(across(where(is.numeric), mean, na.rm = TRUE), .groups = 'drop')

# Create a tibble containing the Class value for each cluster
# Each cluster except for the noisy points (cluster 0) only
# has one value for the class variable so the slice_max()
# function is used to select the most represented Class
# for the noisy points since we are not analyzing the Class 
# of the noise anyways
classVal <- clusterDf |>
  group_by(optics_cluster, Class) |>
  summarise(count = n()) |>
  slice_max(order_by = count, n = 1, with_ties = FALSE)

# Create a tibble containing the satisfaction value for each cluster
satisVal <- clusterDf |>
  group_by(optics_cluster, satisfaction) |>
  summarise(count = n()) |>
  slice_max(order_by = count, n = 1, with_ties = FALSE)

# Join the Class values and satisfaction values with the rest of the average points for each cluster
clusterAvg <- clusterAvg |>
  left_join(classVal |> select(optics_cluster, Class = Class), by = "optics_cluster") |>
  left_join(satisVal |> select(optics_cluster, satisfaction = satisfaction), by = "optics_cluster") |>
  select(optics_cluster, 
         Age, 
         Class, 
         Flight.Distance, 
         satisfaction, 
         pam_cluster)

clusterAvg <- clusterAvg[-1,]

clustCompare <- clusterDf
clustCompare$optics_cluster[clustCompare$optics_cluster == 3] <- 7
clustCompare$optics_cluster[clustCompare$optics_cluster == 4] <- 3
clustCompare$optics_cluster[clustCompare$optics_cluster == 7] <- 4

colnames(clusterAvg) <- c("Optics Cluster",
                          "Age",
                          "Class",
                          "Flight Distance",
                          "Satisfaction",
                          "PAM Cluster")

clusterAvg <- clusterAvg[c("Age",
                           "Class",
                           "Flight Distance",
                           "Satisfaction",
                           "PAM Cluster",
                           "Optics Cluster")]
```

::: panel-tabset
#### OPTICS

```{r optics-cluster-values}
knitr::kable(clusterAvg[,1:4], row.names = FALSE)
```

#### Clusters

```{r}
knitr::kable(clusterAvg[,5:6], row.names = FALSE)
```

#### Assignments

```{r}
clustCompareTable <- table(clustCompare$pam_cluster, clustCompare$optics_cluster)

knitr::kable(clustCompareTable, row.names = TRUE)
```
:::


## Insights

-   Appear to be interesting patterns that show satisfaction only came from:
    -   Older passengers sitting Business class on long flights
    -   Older passengers sitting Eco class on shorter flights
-   Very few passengers flying Eco Plus class were satisfied
    -   Could indicate their higher expectations were not met
-   Improve service to passengers paying for higher classes to increase satisfaction

## Association
* In data mining, association rules identify patterns in a dataset by linking a set of items(anticedents) to another set of items(consequent)
* Antecedent (if) -> Consequent (then)

## Support and Confidence

* Support measures the frequency of an item in the dataset
* Confidence measures the reliability of the rule
* Lift measures the strength of an association 

## Heatmap Correlation

```{r}
cor_matrix <- cor(numeric_data, use = "complete.obs")

cor_melted <- melt(cor_matrix)

ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "#d6eaf8", high = "#4f86c1") + # Blue-to-Green gradient
  labs(
    title = "Correlation Matrix Heatmap",
    x = "Features",
    y = "Features",
    fill = "Correlation"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

```

## Heatmap interpretation

-   There's a strong correlation between *Seat Comfort, Inflight Entertainment, Online Boarding* with *Satisfaction*
-   While operating metrics like *Delays* negatively affect customer satisfaction, their impact is relatively weaker compared to customer-facing metrics

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Prepare the dataset
data_temp <- data

# Define the columns to be recoded
columnsSQE <- c("Online.boarding", "Seat.comfort", "Inflight.entertainment", 
                "Leg.room.service", "On.board.service", "Cleanliness", 
                "Inflight.wifi.service", "Baggage.handling")

# Recode numeric columns into factors: "Poor", "Neutral", "Good"
recode_to_factors <- function(column) {
  factor(ifelse(column %in% c(1, 2), "Poor",
         ifelse(column == 3, "Neutral", "Good")),
         levels = c("Poor", "Neutral", "Good"))
}

for(col in columnsSQE){
  data_temp[[col]] <- recode_to_factors(data_temp[[col]])
}

# Add a new column for satisfaction as a factor
data_temp$satisfaction <- factor(ifelse(data_temp$satisfaction_numerical == 1, 
                                        "Satisfied", "Unsatisfied"),
                                  levels = c("Unsatisfied", "Satisfied"))

# Include the satisfaction column in the subset for transactions
columnsSQE <- c(columnsSQE, "satisfaction")

# Subset only the selected columns for association rule mining
data_subset <- data_temp[, columnsSQE]

# Convert to transactions format
transactions <- as(data_subset, "transactions")
```



## Apriori

* Uses a generate-and-test approach.
* Follows the apriori property

```{r echo=FALSE, include=FALSE, warning=FALSE}
# Run the Apriori algorithm
ars_adjusted <- apriori(transactions, parameter = list(support = 0.02, confidence = 0.6))

# Subset rules where satisfaction = Satisfied is in the rhs
satisfied_rules <- subset(ars_adjusted, rhs %in% "satisfaction=Satisfied")

# Extract top 10 rules by Lift
top_rules_lift <- sort(satisfied_rules, by = "lift")[1:10]
```




## Scatter plot of Supportt vs Confidence for Satisfaction
```{r  warning=FALSE}
plot(
  satisfied_rules,
  method = "scatterplot",
  measure = c("support", "confidence"),
  shading = "lift",
  control = list(
    col = colorRampPalette(c( "#4f86c1", "#d6eaf8"))(1.8)
  )
)
```
## Interpretation of Support Vs Confidence for Satisfaction
* The graph shows a concentration of the higher lift points around the high confidence - low support region
* When selecting the rules, efforts should be made to balance high support rules, with high confidence and high lift rules


## Satisfaction based Rule Network Graph

<iframe src="network_plot_satisfied.html" width="100%" height="550-0px" frameborder="0"></iframe>


```{r warning=FALSE}
# plot(
#   top_rules_lift,
#   method = "graph",
#   engine = "htmlwidget",
#   control = list(
#     nodeCol = "#4ebded",  # Node color (blue-green)
#     edgeCol = "#4f86c1",  # Edge color (light blue)
#     alpha = 0.8           # Transparency for edges
#   )
# )

```
## Interpretation of the network graph

-   Positive features like *Seat comfort, Inflight entertainment, Online boarding, Leg room service* and *On-board service* are connected to multiple rules indicating that they play significant roles in driving satisfaction.
-   Features such as *Baggage handling* and *Cleanliness* play important roles as well but not as much as the ones mentioned before.



## FP Growth

* Uses a divide-and-conquer approach
* Constructs a compact data structure to represent the data

```{r echo=FALSE, include=FALSE, warning=FALSE}
fp_itemsets <- apriori(transactions, parameter = list(support = 0.02, target = "frequent itemsets"))

# Generate rules from the frequent itemsets
fp_rules <- apriori(transactions, parameter = list(support = 0.02, confidence = 0.6))

# Filter rules where satisfaction = Satisfied is in the RHS
unsatisfaction_rules <- subset(fp_rules, rhs %in% "satisfaction=Unsatisfied")

```


## Scatter plot of Support Vs Confidence for Unsatisfaction
```{r warning=FALSE}
# Scatterplot for satisfaction rules
plot(
  unsatisfaction_rules, 
  method = "scatterplot", 
  measure = c("support", "confidence"), 
  shading = "lift",
  control = list(
    col = colorRampPalette(c( "#4f86c1", "#d6eaf8"))(1.8)
  )
)
```
## Interpretation of Support Vs Confidence for Unsatisfaction
* Similar to the satisfaction plot, there is a high concentration of low support and high confidence points.
* Focus should therefore be drawn to rules with moderate lift and higher support .


```{r echo=FALSE, message=FALSE, warning=FALSE}
top_unsatisfaction_rules <- sort(unsatisfaction_rules, by = "lift")[1:10]
```

## Unsatisfaction based Rule Network Graph

<iframe src="network_plot_unsatisfied_fp.html" width="100%" height="550-0px" frameborder="0"></iframe>


```{r}
# plot(
#   top_unsatisfaction_rules, 
#   method = "graph", 
#   engine = "htmlwidget",
#   control = list(
#     nodeCol = "#4ebded",  # Node color (blue-green)
#     edgeCol = "#4f86c1",  # Edge color (light blue)
#     alpha = 0.8           # Transparency for edges
#   )
# )
```

## Interpretation of the Network Graph

* Features such as Poor Leg Service and Poor Wi-Fi Service contribute to unsatisfaction
* Thicker and darker edges represent stronger rules


## Classification Introduction

- This presentation showcases the performance of three classification models: **SVM**, **Decision Tree**, and **Random Forest**.
- For each model, we:
  - Select relevant features based on correlation analysis.
  - Train the model.
  - Evaluate performance using a confusion matrix and accuracy.
  - Visualize results with relevant plots.

```{r, echo=FALSE}
library(tidyverse)
library(caret)
library(e1071)    # For SVM
library(rpart)    # For Decision Tree
library(randomForest) # For Random Forest
library(dplyr)
# Load the training dataset
train_dataset <- read.csv("train.csv")

# Convert "satisfaction" column to factor
train_dataset$satisfaction <- as.factor(train_dataset$satisfaction)

# Handle missing values if any
train_dataset <- train_dataset %>% drop_na()

# Convert categorical variables to factors
categorical_cols <- c("Gender", "Customer.Type", "Type.of.Travel", "Class")
train_dataset[categorical_cols] <- lapply(train_dataset[categorical_cols], as.factor)

# Split data into features and labels
train_x <- train_dataset %>% select(-satisfaction)
train_y <- train_dataset$satisfaction

# Load the test dataset
test_dataset <- read.csv("test.csv")

# Convert "satisfaction" column to factor if it exists
if ("satisfaction" %in% colnames(test_dataset)) {
  test_dataset$satisfaction <- as.factor(test_dataset$satisfaction)
}

# Handle missing values if any
test_dataset <- test_dataset %>% drop_na()

# Convert categorical variables to factors
test_dataset[categorical_cols] <- lapply(test_dataset[categorical_cols], as.factor)

# Split data into features and labels
test_x <- test_dataset %>% select(-satisfaction)
test_y <- if ("satisfaction" %in% colnames(test_dataset)) test_dataset$satisfaction else NULL

```
## Support Vector Machine (SVM)

- SVM uses a linear kernel to classify satisfaction based on selected features.
- Features selected: `Inflight.wifi.service`, `Flight.Distance`, `Online.boarding` and `Seat.comfort`.

```{r}
# Select relevant features
selected_features <- c("Inflight.wifi.service", "Flight.Distance",
                       "Online.boarding", "Seat.comfort", "satisfaction")

# Create subsets for training and testing
# train_dataset_reduced <- train_dataset[, selected_features]
# test_dataset_reduced <- test_dataset[, selected_features]
# 
# # Train SVM model
# svm_model <- svm(satisfaction ~ ., data = train_dataset_reduced, kernel = "linear")
# 
# # Predict and evaluate
# svm_predictions <- predict(svm_model, test_dataset_reduced)
# confusionMatrix(svm_predictions, test_y)
```


## SVM: Confusion Matrix Heatmap

```{r}
library(ggplot2)
library(caret)
library(reshape2)

# # Generate confusion matrix for SVM
# conf_matrix_svm <- confusionMatrix(svm_predictions, test_y)$table
# conf_matrix_svm_melted <- melt(conf_matrix_svm)
# 
# # Heatmap
# ggplot(conf_matrix_svm_melted, aes(x = Prediction, y = Reference, fill = value)) +
#   geom_tile(color = "white") +
#   scale_fill_gradient(low = "lightblue", high = "darkblue") +
#   labs(title = "Confusion Matrix (SVM)", x = "Predicted", y = "Actual", fill = "Frequency") +
#   theme_minimal()
```


## Decision Tree

- Decision Tree classifies satisfaction by recursively splitting the dataset.
- Uses `rpart` library for model creation and visualization.

```{r}
library(rpart)
library(rpart.plot)

# Fit Decision Tree model
tree_model <- rpart(satisfaction ~ ., data = train_dataset, method = "class")

# Predict and evaluate
tree_predictions <- predict(tree_model, test_x, type = "class")
confusionMatrix(tree_predictions, test_y)
```


## Decision Tree: Visualization

```{r}
# Plot the decision tree
rpart.plot(tree_model, type = 4, extra = 104, main = "Decision Tree for Satisfaction")
```


## Random Forest

- Random Forest combines multiple decision trees for robust classification.
- Utilizes `randomForest` library to build the model.

```{r}
library(randomForest)

# Train Random Forest model
rf_model <- randomForest(satisfaction ~ ., data = train_dataset, ntree = 100)

# Predict and evaluate
rf_predictions <- predict(rf_model, test_x)
confusionMatrix(rf_predictions, test_y)
```


## Random Forest: Confusion Matrix Heatmap

```{r}
# Generate confusion matrix for Random Forest
conf_matrix_rf <- confusionMatrix(rf_predictions, test_y)$table
conf_matrix_rf_melted <- melt(conf_matrix_rf)

# Heatmap
ggplot(conf_matrix_rf_melted, aes(x = Prediction, y = Reference, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Confusion Matrix (Random Forest)", x = "Predicted", y = "Actual", fill = "Frequency") +
  theme_minimal()
```


## Feature Importance: Random Forest

- Identifies the features contributing most to classification decisions.

```{r}
# Feature importance plot
importance_df <- data.frame(Feature = rownames(importance(rf_model)), Importance = importance(rf_model)[, 1])

# Bar plot of feature importance
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance from Random Forest", x = "Features", y = "Importance") +
  theme_minimal()
```


## Accuracy Comparison

- The following bar plot compares the accuracy of all three models.

```{r}
# Accuracy comparison
data.frame(
  Model = c("Decision Tree", "Random Forest"),
  Accuracy = c(
    #mean(svm_predictions == test_y),
    mean(tree_predictions == test_y),
    mean(rf_predictions == test_y)
  )
) %>%
  ggplot(aes(x = reorder(Model, -Accuracy), y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity", color = "black") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Model Accuracy Comparison", x = "Model", y = "Accuracy") +
  theme_minimal()
```

## Conclusion

- **SVM** performs well with fewer features.
- **Decision Tree** provides easy interpretability.
- **Random Forest** excels in feature importance and overall accuracy.
- Consider model choice based on application requirements and computational resources.

