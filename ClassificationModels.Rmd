---
title: "Code"
author: "Agastya Deshraju, Nick Ferrante, Jasdeep Singh Jhajj"
date: "2024-12-09"
output: html_document
---

# Installing and Loading Packages

```{r}
if (!require("pacman")) 
  install.packages("pacman")

pacman::p_load(tidyverse,
               dplyr,
               scales,
               arules,
               arulesViz,
               viridis,
               reshape2) 

```

###  Reading the Data

The dataset train.csv is loaded into the data variable using the read.csv().
```{r}
data <- read.csv('train.csv')
data |> head()

```

### Checking for Missing Values

```{r}
colSums(is.na(data))
```

## Handling Missing Data
 Here we replace all missing (NA) values that exist in "Arrival.Delay.in.Minutes" with zeros assuming no delay.
 
```{r}
data[is.na(data)] <- 0

colSums(is.na(data))
```

## Encoding Satisfaction as Binary
The satisfaction column is recoded to have only two values: "satisfied" or "unsatisfied". This makes it easier to model in binary classification tasks.

```{r}
data$satisfaction <- ifelse(data$satisfaction == "satisfied", "satisfied", "unsatisfied")
data |>head()
```

## Creating a Numeric Representation of Satisfaction for Computing Correlation Matrix

r
Copy code


```{r}
data$satisfaction_numerical <- ifelse(data$satisfaction == "satisfied", 1, 0)
```

```{r}
numeric_data <- data[, -c(1,2,3,4,6,7,25)]

numeric_data
```


This code computes the correlation matrix for the numeric data, which shows how strongly features are related to each other. The melt() function reshapes the correlation matrix for visualization.
```{r}
cor_matrix <- cor(numeric_data, use = "complete.obs")

cor_melted <- melt(cor_matrix)
```

## Plotting Correlation Matrix Heatmap

```{r}
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "#ffebeb", high = "#ff0000") +
  labs(
    title = "Correlation Matrix Heatmap",
    x = "Features",
    y = "Features",
    fill = "Correlation"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```


# Bar Plot for Categorical Variables
Visualize how categorical features like Class relate to Satisfaction

```{r}
library(ggplot2)

# Example: Bar plot for Class and Satisfaction
ggplot(data, aes(x = Class, fill = satisfaction)) +
  geom_bar(position = "fill") +
  labs(title = "Satisfaction Levels by Class", y = "Proportion") +
  theme_minimal()

```


```{r}
# Density plot for Flight Distance
ggplot(data, aes(x = Flight.Distance, fill = satisfaction)) +
  geom_density(alpha = 0.5) +
  labs(title = "Density Plot of Flight Distance by Satisfaction", x = "Flight Distance", y = "Density") +
  theme_minimal()

```

```{r}
# Stacked Bar Chart for Customer Type
ggplot(data, aes(x = Customer.Type, fill = satisfaction)) +
  geom_bar(position = "fill") +
  labs(title = "Satisfaction Levels by Customer Type", y = "Proportion", x = "Customer Type") +
  theme_minimal()

```

# CODE

```{r}
library(tidyverse)
library(caret)
library(e1071)    # For SVM
library(rpart)    # For Decision Tree
library(randomForest) # For Random Forest
library(dplyr)

```

# Loading and Preprocessing Training Data

```{r}
# Load the training dataset
train_dataset <- read.csv("train.csv")

# Convert "satisfaction" column to factor
train_dataset$satisfaction <- as.factor(train_dataset$satisfaction)

# View the dataset structure
str(train_dataset)
```
# Data Preprocessing for Training Dataset

```{r}
# Handle missing values if any
train_dataset <- train_dataset %>% drop_na()

# Convert categorical variables to factors
categorical_cols <- c("Gender", "Customer.Type", "Type.of.Travel", "Class")
train_dataset[categorical_cols] <- lapply(train_dataset[categorical_cols], as.factor)

# Split data into features and labels
train_x <- train_dataset %>% select(-satisfaction)
train_y <- train_dataset$satisfaction

```

# Loading and Preprocessing Test Data

```{r}
# Load the test dataset
test_dataset <- read.csv("test.csv")

# Convert "satisfaction" column to factor if it exists
if ("satisfaction" %in% colnames(test_dataset)) {
  test_dataset$satisfaction <- as.factor(test_dataset$satisfaction)
}

# Handle missing values if any
test_dataset <- test_dataset %>% drop_na()

# Convert categorical variables to factors
test_dataset[categorical_cols] <- lapply(test_dataset[categorical_cols], as.factor)

# Split data into features and labels
test_x <- test_dataset %>% select(-satisfaction)
test_y <- if ("satisfaction" %in% colnames(test_dataset)) test_dataset$satisfaction else NULL

```


# SVM

In the code below we train a Support Vector Machine (SVM) model using only the selected features based on correlation analysis(only some of the correlated columns have been used due to computational challenges). It then predicts the satisfaction label and evaluates the model using a confusion matrix.

```{r}
# Select only the relevant features based on correlation analysis
selected_features <- c("Inflight.wifi.service", "Flight.Distance",
                       "Online.boarding", "Seat.comfort", "satisfaction")

# Create a subset of the dataset with selected features
train_dataset_reduced <- train_dataset[, selected_features]
test_dataset_reduced <- test_dataset[, selected_features]

# Train SVM with the reduced feature set
svm_model <- svm(satisfaction ~ ., data = train_dataset_reduced, kernel = "linear")

# Predict on the test dataset
svm_predictions <- predict(svm_model, test_dataset_reduced)

# Evaluate model performance
confusionMatrix(svm_predictions, test_y)
```




# Decision Tree
We create a decision tree using the rpart method and evaluates its performance by generating predictions and plotting the decision tree structure.

```{r}

library(rpart)
library(rpart.plot)

# Fit a decision tree
tree_model <- rpart(satisfaction ~ ., data = train_dataset, method = "class")

# Predict on test dataset
tree_predictions <- predict(tree_model, test_x, type = "class")

# Evaluate model
confusionMatrix(tree_predictions, test_y)

# Plot the tree
rpart.plot(tree_model, type = 4, extra = 104, main = "Decision Tree for Satisfaction")

```

# Random Forest
A Random Forest model is trained using the training dataset, and its performance is evaluated by comparing the predicted labels to the actual labels using a confusion matrix.

```{r}
# Train Random Forest model
rf_model <- randomForest(satisfaction ~ ., data = train_dataset, ntree = 100)

# Predict on test dataset
rf_predictions <- predict(rf_model, test_x)

# Evaluate model if true labels are available
confusionMatrix(rf_predictions, test_y)


```

```{r}
library(caret)
library(reshape2)

# Confusion Matrix for Random Forest
conf_matrix_rf <- confusionMatrix(rf_predictions, test_y)$table

# Convert to dataframe for heatmap
conf_matrix_rf_melted <- melt(conf_matrix_rf)

# Heatmap
ggplot(conf_matrix_rf_melted, aes(x = Prediction, y = Reference, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Confusion Matrix (Random Forest)", x = "Predicted", y = "Actual", fill = "Frequency") +
  theme_minimal()

```
## Feature Importance from Random Forest
This code visualizes the feature importance from the Random Forest model using a bar plot. It helps to identify which features contribute the most to model prediction.

```{r}
# Feature importance from Random Forest
importance_df <- data.frame(Feature = rownames(importance(rf_model)), Importance = importance(rf_model)[, 1])

# Bar plot of feature importance
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance from Random Forest", x = "Features", y = "Importance") +
  theme_minimal()

```



```{r}
# # Collect accuracy for models (if true labels are available)
  results <- data.frame(
    Model = c("SVM", "Decision Tree", "Random Forest"),
    Accuracy = c(
      mean(svm_predictions == test_y),
      mean(tree_predictions == test_y),
      mean(rf_predictions == test_y)
    )
  )
  print(results)

```



## Accuracy Comparison

```{r}
# Accuracy comparison
accuracy_df <- data.frame(
  Model = c("SVM", "Decision Tree", "Random Forest"),
  Accuracy = c(
    mean(svm_predictions == test_y),
    mean(tree_predictions == test_y),
    mean(rf_predictions == test_y)
  )
)

# Bar plot of accuracies, ordered by accuracy
accuracy_df$Model <- factor(accuracy_df$Model, levels = accuracy_df$Model[order(accuracy_df$Accuracy)])

ggplot(accuracy_df, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity", color = "black") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Model Accuracy Comparison", x = "Model", y = "Accuracy") +
  theme_minimal()


```

